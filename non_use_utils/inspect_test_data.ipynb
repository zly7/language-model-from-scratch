{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "preprocessed_splits = load_from_disk(\"../wikitext-103-preprocessed-ws-notext-gpt2-128-wtest-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014634\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_splits train :  {'input_ids': [20665, 14064, 710, 21566, 22863, 837, 257, 3830, 32803, 283, 286, 262, 2908, 29350, 70, 341, 390, 15085, 79, 385, 837, 11406, 319, 20111, 5451, 319, 3269, 362, 837, 1248, 2414, 837, 319, 262, 1987, 400, 1110, 286, 465, 12928, 422, 3254, 1845, 64, 8836, 568, 764, 679, 373, 284, 3520, 319, 20111, 5451, 329, 5193, 1933, 837, 17138, 2890, 663, 17622, 764, 679, 2630, 281, 1848, 286, 465, 2652, 287, 543, 339, 3136, 465, 9412, 286, 262, 17255, 326, 614, 1058, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [1439, 286, 262, 47341, 3466, 547, 5924, 287, 14523, 6811, 947, 656, 644, 484, 3417, 355, 366, 3024, 406, 57, 82, 366, 837, 475, 262, 1633, 3653, 547, 11472, 416, 262, 12040, 290, 12245, 286, 262, 2323, 290, 3098, 2488, 12, 31, 6215, 2046, 484, 550, 7452, 1201, 262, 9992, 2230, 2540, 764, 770, 3017, 407, 691, 1402, 5101, 2046, 290, 262, 4271, 973, 2242, 3020, 837, 5214, 3020, 290, 7632, 3020, 49445, 6541, 837, 475, 262, 7600, 3020, 290, 1802, 3020, 49445, 6541, 837, 1863, 351, 262, 2116, 2488, 12, 31, 40390, 837, 13428, 2488, 12, 31, 17455, 7632, 3020, 25734, 2485, 764, 4042, 6452, 284, 262, 3399, 547, 262, 1588, 4931, 286, 14719, 2488, 12, 31, 362, 28844, 82, 837, 290, 329, 262, 717, 640, 262], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "preprocessed_splits train :  {'input_ids': [796, 796, 796, 796, 21733, 268, 268, 290, 3738, 15448, 79, 357, 1248, 5999, 784, 9849, 1267, 796, 796, 796, 796, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [383, 4735, 837, 4755, 4645, 286, 257, 31733, 318, 1900, 355, 262, 29984, 764, 36238, 560, 17751, 72, 389, 13160, 286, 281, 45541, 14755, 286, 3881, 837, 8977, 837, 1660, 4771, 837, 290, 12912, 21678, 884, 355, 6588, 17556, 837, 6588, 937, 28885, 837, 25006, 837, 290, 44957, 764, 1081, 884, 837, 484, 389, 2968, 306, 3417, 355, 366, 11841, 6729, 21591, 366, 706, 8559, 854, 18793, 705, 82, 2746, 764, 2102, 837, 617, 401, 1039, 743, 423, 257, 2440, 8977, 2695, 837, 3756, 606, 284, 307, 1444, 366, 30284, 13647, 21591, 366, 764, 4992, 5952, 287, 1946, 5644, 326, 401, 1039, 389, 588, 366, 2769, 23018, 4771, 8566, 366, 837, 287, 326, 511, 16649, 389, 7042, 286, 15715, 31578, 500, 4771, 7668, 351, 10469, 16439, 837, 981], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "preprocessed_splits train :  {'input_ids': [796, 796, 796, 7439, 2677, 796, 796, 796, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [309, 3832, 88, 12334, 7278, 5166, 572, 422, 262, 2479, 286, 530, 614, 837, 290, 2652, 1978, 287, 257, 3221, 43173, 10877, 2776, 329, 1204, 764, 1052, 4920, 5166, 705, 82, 7674, 318, 13768, 614, 2488, 12, 31, 2835, 290, 9456, 351, 1310, 837, 611, 597, 837, 18645, 1487, 422, 614, 284, 614, 764, 383, 5166, 1650, 287, 3002, 319, 257, 8478, 1969, 284, 257, 5509, 21427, 1141, 262, 1110, 837, 290, 3221, 686, 455, 13869, 422, 2901, 284, 3267, 764, 5564, 455, 278, 12334, 7278, 743, 307, 5071, 290, 366, 7251, 3077, 366, 416, 1402, 10087, 1141, 262, 1110, 837, 475, 484, 7685, 8856, 262, 30497, 764, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [12168, 4172, 286, 367, 2357, 32563, 64, 389, 1660, 11316, 837, 16573, 284, 281, 37115, 12263, 837, 884, 355, 262, 1660, 8848, 3653, 357, 2744, 844, 31718, 1267, 837, 1660, 47394, 507, 357, 15310, 31718, 1267, 837, 290, 736, 2032, 36904, 357, 1892, 261, 478, 31718, 1267, 764, 1119, 389, 4632, 35341, 837, 290, 423, 7405, 16573, 355, 14098, 829, 284, 1037, 262, 5044, 1445, 832, 262, 1660, 764, 383, 16723, 8135, 8605, 393, 1660, 965, 4157, 357, 13573, 6058, 3609, 1267, 389, 635, 3917, 351, 1660, 837, 475, 779, 262, 4417, 12097, 286, 5055, 1660, 284, 1394, 606, 2029, 262, 4417, 2162, 484, 2291, 262, 5417, 1341, 8605, 287, 262, 34306, 11023, 672, 689, 837, 262, 691, 4988, 16050, 1448, 286, 19435, 764, 220, 198, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [347, 13, 264, 494, 36575, 4178, 12723, 837, 1248, 2414, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [412, 13, 40395, 318, 6777, 973, 355, 257, 2746, 26433, 287, 24559, 1435, 3640, 764, 22449, 30829, 21245, 357, 304, 13, 70, 13, 412, 13, 40395, 509, 1065, 1267, 389, 880, 2488, 12, 31, 16573, 284, 262, 14010, 2858, 837, 290, 837, 5023, 4295, 2488, 12, 31, 2099, 21245, 837, 423, 2626, 511, 2694, 284, 22191, 287, 262, 45426, 764, 4650, 14010, 21245, 4425, 511, 2694, 284, 1296, 13401, 10379, 907, 764, 2312, 3033, 1805, 4295, 2488, 12, 31, 2099, 21245, 422, 30869, 290, 584, 5931, 3434, 837, 475, 2421, 257, 1588, 20705, 286, 2568, 290, 2587, 4133, 764, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "preprocessed_splits train :  {'input_ids': [1675, 779, 691, 883, 9021, 290, 5696, 543, 547, 2829, 837, 20823, 6789, 837, 407, 13568, 837, 290, 48287, 764, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"preprocessed_splits train : \", preprocessed_splits[\"train\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['overflowing_tokens', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_splits[\"test\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # 论文有说是increase 到1024\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, GPT2Tokenizer, GPT2Config\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"./tokenizer_save/tokenizer-gpt2-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_save/tokenizer-gpt2-128/tokenizer_config.json',\n",
       " './tokenizer_save/tokenizer-gpt2-128/special_tokens_map.json',\n",
       " './tokenizer_save/tokenizer-gpt2-128/vocab.json',\n",
       " './tokenizer_save/tokenizer-gpt2-128/merges.txt',\n",
       " './tokenizer_save/tokenizer-gpt2-128/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(f\"./tokenizer_save/tokenizer-{tokenizer.name_or_path}-{tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "preprocessed_splits[\"test\"] = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceca8d3533d4908aa939b5a1f0ccbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits[\"test\"].save_to_disk(\"wikitext-103-raw-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(preprocessed_splits[\"test\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4358\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"test\"]))\n",
    "print(type(preprocessed_splits[\"test\"][0][\"overflowing_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是一套文本类test的数据集预处理组合\n",
    "主要实现文本转token，batch长度，test的标准答案overflowing token转化成ID，并且要能有比较长的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ba496d90cb8b690f_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "# 这里存在的问题就是overflowing_tokens 长短不一，没办法batch\n",
    "def preprocess_function(example):\n",
    "    return_dic = tokenizer.__call__(example[\"text\"], padding=\"max_length\", truncation=True,   # 在这就不带文本了\n",
    "        max_length=tokenizer.model_max_length, return_overflowing_tokens=True) \n",
    "\n",
    "    # if len(return_dic[\"overflowing_tokens\"]) > 0:\n",
    "    #     return_dic[\"overflow_text\"] = tokenizer.decode(return_dic[\"overflowing_tokens\"])\n",
    "    # else:\n",
    "    #     return_dic[\"overflow_text\"] = \"\"\n",
    "    # return_dic[\"prompt\"] = tokenizer.decode(return_dic[\"input_ids\"])\n",
    "    return return_dic\n",
    "#Apply preprocessing to dataset\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(preprocess_function,batched=True, remove_columns=[\"text\"],num_proc=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-99423aefded79e6e_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "def filter_function(example):  # 就没有大于300的\n",
    "    return len(example['overflowing_tokens']) > 20 and len(example['overflowing_tokens']) < 300 \n",
    "\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].filter(filter_function,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(preprocessed_splits[\"test\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3167941d1646f6852e456a4357b493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function_truncate(example, max_over_length=128):\n",
    "    # if isinstance(example[\"overflowing_tokens\"], list):\n",
    "    #     example[\"overflowing_tokens\"] = example[\"overflowing_tokens\"].truncate(max_over_length)\n",
    "    # else:\n",
    "    # print(\"type: \",type(example[\"overflowing_tokens\"]))\n",
    "    def complete_list(lst, length, value=tokenizer.eos_token_id):\n",
    "        l = len(lst)\n",
    "        return lst + [value] * (length - l)\n",
    "    if len(example[\"overflowing_tokens\"]) > max_over_length:\n",
    "        example[\"overflowing_tokens\"] = example[\"overflowing_tokens\"][:max_over_length]\n",
    "    else:\n",
    "        example[\"overflowing_tokens\"] = complete_list(example[\"overflowing_tokens\"], max_over_length)\n",
    "    return example\n",
    "\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(preprocess_function_truncate, batched=False, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"overflowing_tokens_ids\"])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"num_truncated_tokens\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最开始没看懂这个batch，用下面的函数验证\n",
    "# def preprocess_function_truncate(example, max_over_length=128):\n",
    "#     print(\"type: \",type(example[\"overflowing_tokens\"]))\n",
    "#     print(\"length: \",len(example[\"overflowing_tokens\"]))\n",
    "#     print(\"type: \",type(example[\"overflowing_tokens\"][0]))\n",
    "#     print(\"length: \",len(example[\"overflowing_tokens\"][0]))\n",
    "#     return example\n",
    "# preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(preprocess_function_truncate, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def complete_list(lst, length, value=tokenizer.eos_token_id):\n",
    "#     \"\"\"\n",
    "#     补全list到指定长度\n",
    "#     :param lst: 原list\n",
    "#     :param length: 指定长度\n",
    "#     :param value: 补全的值，默认为None\n",
    "#     :return: 补全后的list\n",
    "#     \"\"\"\n",
    "#     l = len(lst)\n",
    "#     return lst + [value] * (length - l)\n",
    "\n",
    "# for i in range(len(preprocessed_splits[\"test\"])):\n",
    "#     if(len(preprocessed_splits[\"test\"][i][\"overflowing_tokens\"]) >= 128):\n",
    "#         preprocessed_splits[\"test\"][i][\"overflowing_tokens\"] = preprocessed_splits[\"test\"][i][\"overflowing_tokens\"][:128]\n",
    "#     else:\n",
    "#         preprocessed_splits[\"test\"][i][\"overflowing_tokens\"] = complete_list(preprocessed_splits[\"test\"][i][\"overflowing_tokens\"], 128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(preprocessed_splits[\"test\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns(\"num_truncated_tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns(\"overflowing_tokens_truncate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a181ca4959d485cb488f5497a24f92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1014634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ca6a9c48104039a8459f59bf876629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/112738 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95152e715a34d388a11f306f90f845c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits.save_to_disk(\"wikitext-103-preprocessed-ws-notext-gpt2-128-wtest-v2\")#wikitext-103-preprocessed-ws-notext-gpt2-128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
