# 舍弃这个原因主要是每个模型的block基本都需要定制，而且这个模型很短
# import torch
# import torch.nn as nn
# from torch.nn import functional as F
# import math
# from .bb_attention import CausalSelfAttention
# from .bb_basic import LayerNorm, MLP

# class Block(nn.Module):

#     def __init__(self, config):
#         super().__init__()
#         self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
#         self.attn = CausalSelfAttention(config)
#         self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
#         self.mlp = MLP(config)

#     def forward(self, x):
#         x = x + self.attn(self.ln_1(x))
#         x = x + self.mlp(self.ln_2(x))
#         return x