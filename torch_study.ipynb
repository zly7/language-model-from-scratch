{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3, 0, 4],\n",
      "        [3, 3, 4, 1],\n",
      "        [3, 2, 1, 0]])\n",
      "tensor([[[0.1337, 0.3494, 0.0254, 0.3228, 0.1687],\n",
      "         [0.0668, 0.2152, 0.2133, 0.4478, 0.0569],\n",
      "         [0.1909, 0.0941, 0.1236, 0.3587, 0.2328],\n",
      "         [0.5949, 0.0463, 0.1171, 0.0431, 0.1986]],\n",
      "\n",
      "        [[0.2478, 0.6146, 0.0830, 0.0219, 0.0328],\n",
      "         [0.5875, 0.1575, 0.0884, 0.0212, 0.1455],\n",
      "         [0.3837, 0.0377, 0.1511, 0.3245, 0.1031],\n",
      "         [0.1429, 0.4570, 0.0215, 0.3211, 0.0575]],\n",
      "\n",
      "        [[0.1262, 0.0491, 0.0708, 0.1700, 0.5838],\n",
      "         [0.0908, 0.2909, 0.1474, 0.4296, 0.0413],\n",
      "         [0.0511, 0.3814, 0.2524, 0.0658, 0.2492],\n",
      "         [0.1168, 0.5151, 0.1010, 0.1983, 0.0689]]])\n",
      "torch.Size([3, 4])\n",
      "tensor([[0.1687, 0.4478, 0.1909, 0.1986],\n",
      "        [0.0219, 0.0212, 0.1031, 0.4570],\n",
      "        [0.1700, 0.1474, 0.3814, 0.1168]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "b=3\n",
    "t=4\n",
    "vocab_size=5\n",
    "logits = torch.randn((b, t, vocab_size))\n",
    "# print(logits)\n",
    "input_ids = torch.randint(low=0, high=vocab_size, size=(b, t))\n",
    "print(input_ids)\n",
    "# Get the probabilities using softmax\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(probs)\n",
    "# Flatten the probabilities and input_ids tensors\n",
    "probs = probs.view(b*t, -1)\n",
    "input_ids = input_ids.view(b*t)\n",
    "\n",
    "# Get the probabilities for the input_ids\n",
    "input_probs = torch.gather(probs, dim=1, index=input_ids.unsqueeze(1))\n",
    "\n",
    "# Reshape the probabilities tensor to match the shape of input_ids\n",
    "input_probs = input_probs.view(b, t)\n",
    "\n",
    "print(input_probs.shape)  # (b, t)\n",
    "print(input_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, UserDict\n",
    "class A(OrderedDict):\n",
    "    def __setattr__(self, name, value):\n",
    "        if name in self.keys() and value is not None:\n",
    "            # Don't call self.__setitem__ to avoid recursion errors\n",
    "            super().__setitem__(name, value)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A()\n",
    "a.__setattr__(\"a\",12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(i)\n",
    "    i = i + 4\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6338, -0.5227]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "modelt = torch.nn.Linear(2,1)\n",
    "print(modelt.weight.data)\n",
    "modelt.weight.data = torch.tensor([[ 0.6338, -0.5227]]) # 保证每次实验一样\n",
    "modelt.bias.data = torch.tensor([0.5906])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1790],\n",
      "        [0.4013],\n",
      "        [0.6235]], grad_fn=<AddmmBackward0>)\n",
      "tensor(2.9593, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = modelt(torch.tensor([[1,2],[3,4],[5,6]],dtype=torch.float32))\n",
    "print(output)\n",
    "loss = torch.nn.MSELoss()\n",
    "target = torch.tensor([[1],[2],[3]],dtype=torch.float32)\n",
    "print(loss(output,target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_r = loss(output,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.986666666666667\n",
      "-3.2000000000000006\n",
      "-11.733333333333334\n",
      "-14.933333333333332\n"
     ]
    }
   ],
   "source": [
    "print((0.8**2+1.6**2+2.4**2)/3)#loss\n",
    "print((-0.8-1.6-2.4)*2/3)\n",
    "print((-0.8-1.6*3-2.4*5)*2/3)#grad,sum(2*(y-t)*x)/n\n",
    "print((-0.8*2-1.6*4-2.4*6)*2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7926, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7926, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in loss.parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.6338, -0.5227]], requires_grad=True))\n",
      "grad: tensor([[-11.6665, -14.8640]])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.5906], requires_grad=True))\n",
      "grad: tensor([-3.1975])\n"
     ]
    }
   ],
   "source": [
    "for i in modelt.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in modelt.named_parameters():\n",
    "    i[1].grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss(reduction=\"sum\")\n",
    "target = torch.tensor([[1],[2],[3]],dtype=torch.float32)\n",
    "output = modelt(torch.tensor([[1,2],[3,4],[5,6]],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.6338, -0.5227]], requires_grad=True))\n",
      "grad: tensor([[-34.9994, -44.5919]])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.5906], requires_grad=True))\n",
      "grad: tensor([-9.5925])\n"
     ]
    }
   ],
   "source": [
    "loss_r = loss(output,target)\n",
    "loss_r.backward()\n",
    "for i in modelt.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "modelt2 = torch.nn.Linear(2,3)\n",
    "modelt2.weight.data = torch.tensor([[ 0.0981, -0.2518],\n",
    "        [ 0.6486, -0.2012],\n",
    "        [-0.1067,  0.6422]],dtype=torch.float32)\n",
    "modelt2.bias.data = torch.tensor([0.1,0.2,0.3],dtype=torch.float32)\n",
    "output = modelt2(torch.tensor([[1,2]],dtype=torch.float32))\n",
    "target = torch.tensor([[1,2,3]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3054,  0.4463,  1.4778]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1245333333333334\n"
     ]
    }
   ],
   "source": [
    "print((1.3**2+1.56**2+1.5**2)/3)#loss\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1451, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.0981, -0.2518],\n",
      "        [ 0.6486, -0.2012],\n",
      "        [-0.1067,  0.6422]], requires_grad=True))\n",
      "grad: tensor([[-0.8703, -1.7405],\n",
      "        [-1.0358, -2.0716],\n",
      "        [-1.0148, -2.0297]])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.1000, 0.2000, 0.3000], requires_grad=True))\n",
      "grad: tensor([-0.8703, -1.0358, -1.0148])\n"
     ]
    }
   ],
   "source": [
    "for i in modelt2.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(2*(1.3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "modelt2 = torch.nn.Linear(2,3)\n",
    "modelt3 = torch.nn.Linear(1,2)\n",
    "modelt2.weight.data = torch.tensor([[ 0.1, 0.2],\n",
    "        [ 0.3, 0.4],\n",
    "        [0.5,  0.6]],dtype=torch.float32)\n",
    "output = modelt2(modelt3(torch.tensor([1],dtype=torch.float32)))\n",
    "target = torch.tensor([[1,2,3]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5920, 0.2637, 0.9637], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[0.0202],\n",
      "        [0.2476]], requires_grad=True))\n",
      "grad: tensor([[-1.0532],\n",
      "        [-1.3319]])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.3460, 0.1005], requires_grad=True))\n",
      "grad: tensor([-1.0532, -1.3319])\n"
     ]
    }
   ],
   "source": [
    "for i in modelt3.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里就计算下第二层关于b的梯度，dl/db1=dl/dx1 = ((0.59-1)*0.1+(0.26-2)*0.3+(0.97-3)*0.5)*2/3(最后的2/3是损失函数带的倍率)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0518948000000001\n",
      "-1.3305336\n"
     ]
    }
   ],
   "source": [
    "print(((0.59-1)*0.1+(0.26-2)*0.3+(0.97-3)*0.5)*0.6666)\n",
    "print(((0.59-1)*0.2+(0.26-2)*0.4+(0.97-3)*0.6)*0.6666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面说明宽度确实有可能导致爆炸，dy/dx会有很多关于W的叠加，所以PyTorch的默认初始方差是根据模型的输入和输出维度自动计算得出的。具体来说，PyTorch使用以下公式计算默认初始方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "modelc = torch.nn.Conv2d(1,1,3)\n",
    "modelc.weight.data = torch.tensor([[[[ 0.1, 0.2,0.3],\n",
    "        [ 0.4, 0.5,0.6],\n",
    "        [0.7,  0.8,0.9]]]],dtype=torch.float32)\n",
    "modelc.bias.data = torch.tensor([0.1],dtype=torch.float32)\n",
    "output = modelc(torch.tensor([[[[1,2,3],[4,5,6],[7,8,9]]]],dtype=torch.float32))\n",
    "target = torch.tensor([[[[2]]]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[28.6000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor(707.5600, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.5\n"
     ]
    }
   ],
   "source": [
    "print((0.1*0.1+0.2*0.2+0.3*0.3+0.4*0.4+0.5*0.5+0.6*0.6+0.7*0.7+0.8*0.8+0.9*0.9)*10+0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[[[0.1000, 0.2000, 0.3000],\n",
      "          [0.4000, 0.5000, 0.6000],\n",
      "          [0.7000, 0.8000, 0.9000]]]], requires_grad=True))\n",
      "grad: tensor([[[[ 53.2000, 106.4000, 159.6000],\n",
      "          [212.8000, 266.0000, 319.2000],\n",
      "          [372.4000, 425.6000, 478.8000]]]])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.1000], requires_grad=True))\n",
      "grad: tensor([53.2000])\n"
     ]
    }
   ],
   "source": [
    "for i in modelc.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.2\n",
      "478.8\n"
     ]
    }
   ],
   "source": [
    "print(2*(28.6-2))\n",
    "print(2*(28.6-2)*9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "modelc = torch.nn.Conv2d(1,1,3)\n",
    "modelc.weight.data = torch.tensor([[[[ 0.1, 0.2,0.3],\n",
    "        [ 0.4, 0.5,0.6],\n",
    "        [0.7,  0.8,0.9]]]],dtype=torch.float32)\n",
    "modelc.bias.data = torch.tensor([0.1],dtype=torch.float32)\n",
    "output = modelc(torch.tensor([[[[1,2,3,4],[4,5,6,7],[7,8,9,10]]]],dtype=torch.float32))\n",
    "target = torch.tensor([[[[20,30]]]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[28.6000, 33.1000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor(41.7850, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.78500000000002\n"
     ]
    }
   ],
   "source": [
    "print(((28.6-20)**2+(33.1-30)**2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[[[0.1000, 0.2000, 0.3000],\n",
      "          [0.4000, 0.5000, 0.6000],\n",
      "          [0.7000, 0.8000, 0.9000]]]], requires_grad=True))\n",
      "grad: tensor([[[[ 14.8000,  26.5000,  38.2000],\n",
      "          [ 49.9000,  61.6000,  73.3000],\n",
      "          [ 85.0000,  96.7000, 108.4000]]]])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.1000], requires_grad=True))\n",
      "grad: tensor([11.7000])\n"
     ]
    }
   ],
   "source": [
    "for i in modelc.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.700000000000003\n",
      "14.800000000000004\n"
     ]
    }
   ],
   "source": [
    "print(2*((28.6-20)+(33.1-30))/2)\n",
    "print(2*((28.6-20)+(33.1-30)*2)/2) # w对于x的也是叠加，左边乘2是平方落下来，右边除2是损失函数对应的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "maxpoolmodel = torch.nn.MaxPool2d(2,2)\n",
    "temp_tensor = torch.tensor([[[[1,2,3,4],[4,5,6,7],[7,8,9,10],[11,12,13,14]]]],dtype=torch.float32,requires_grad=True)\n",
    "output = maxpoolmodel(temp_tensor)\n",
    "\n",
    "target = torch.tensor([[[[4,6],[11,13]]]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.,  7.],\n",
      "          [12., 14.]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "tensor(1., grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5000, 0.0000, 0.5000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5000, 0.0000, 0.5000]]]])\n"
     ]
    }
   ],
   "source": [
    "print(temp_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "layern = torch.nn.LayerNorm(3)\n",
    "layern.weight.data = torch.tensor([0.1,0.2,0.3],dtype=torch.float32)\n",
    "layern.bias.data = torch.tensor([0.4,0.5,0.6],dtype=torch.float32)\n",
    "temp_tensor = torch.tensor([[1,2,3],[3,4,6],[2,6,9]],dtype=torch.float32,requires_grad=True)\n",
    "output = layern(temp_tensor)\n",
    "target = torch.tensor([[0.1,0.2,0.3],[0.3,0.4,0.5],[0.5,0.6,0.7]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2775, 0.5000, 0.9674],\n",
      "        [0.2931, 0.4465, 1.0009],\n",
      "        [0.2721, 0.5232, 0.9487]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor(0.1044, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0044, -0.0089,  0.0044],\n",
      "        [ 0.0031, -0.0046,  0.0015],\n",
      "        [ 0.0011, -0.0025,  0.0014]])\n",
      "('weight', Parameter containing:\n",
      "tensor([0.1000, 0.2000, 0.3000], requires_grad=True))\n",
      "grad: tensor([ 0.0181, -0.0047,  0.3946])\n",
      "('bias', Parameter containing:\n",
      "tensor([0.4000, 0.5000, 0.6000], requires_grad=True))\n",
      "grad: tensor([-0.0127,  0.0600,  0.3149])\n"
     ]
    }
   ],
   "source": [
    "print(temp_tensor.grad)\n",
    "for i in layern.named_parameters():\n",
    "    print(i)\n",
    "    print(\"grad: \" + str(i[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.012733333333333319\n",
      "tensor(-1.2247)\n",
      "tensor(-1.0690)\n",
      "tensor(-1.2787)\n"
     ]
    }
   ],
   "source": [
    "print(((0.2775-0.1)+(0.2931-0.3)+(0.2721-0.5))*2/9) # 后面这个除9是loss function 带来的\n",
    "# 先把平均值算了，\n",
    "def compute_p1(a,index,eps):\n",
    "    return (a[index]-torch.mean(a))/torch.sqrt(torch.var(input=a,unbiased=False)+eps)  # 这里是有偏估计\n",
    "    # return (a[index]-torch.mean(a))/torch.sqrt(torch.var(input=a,correction=0)+eps)\n",
    "print(compute_p1(torch.tensor([1,2,3],dtype=torch.float32),0,1e-5))\n",
    "print(compute_p1(torch.tensor([3,4,6],dtype=torch.float32),0,1e-5))\n",
    "print(compute_p1(torch.tensor([2,6,9],dtype=torch.float32),0,1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(-0.2673)\n",
      "tensor(0.1162)\n"
     ]
    }
   ],
   "source": [
    "print(compute_p1(torch.tensor([1,2,3],dtype=torch.float32),1,1e-5))\n",
    "print(compute_p1(torch.tensor([3,4,6],dtype=torch.float32),1,1e-5))\n",
    "print(compute_p1(torch.tensor([2,6,9],dtype=torch.float32),1,1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01809057333333332\n",
      "-0.0006351066666666664\n"
     ]
    }
   ],
   "source": [
    "# print(((0.2775-0.1)*-1+(0.2931-0.3)*-0.8729+(0.2721-0.5)*-1.0441)*2/9)\n",
    "# print(((0.5-0.2)*0+(0.4465-0.4)*-0.2182+(0.5232-0.6)*-0.0949)*2/9)\n",
    "print(((0.2775-0.1)*-1.2247+(0.2931-0.3)*-1.0690+(0.2721-0.5)*-1.2787)*2/9)\n",
    "# print(((0.5-0.2)*0+(0.4465-0.4)*-0.2182+(0.5232-0.6)*-0.0949)*2/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([[-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "layern = torch.nn.LayerNorm(3)\n",
    "# temp_tensor = torch.tensor([[1,2,3],[3,4,6],[2,6,9]],dtype=torch.float32,requires_grad=True)\n",
    "temp_tensor = torch.tensor([[1,2,3]],dtype=torch.float32,requires_grad=True)\n",
    "print(torch.var(torch.tensor([1,2,3],dtype=torch.float32)))\n",
    "output = layern(temp_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batchn = torch.nn.BatchNorm2d(1) # 这个3的意思是channel的数量\n",
    "batchn.weight.data = torch.tensor([0.1],dtype=torch.float32)\n",
    "batchn.bias.data = torch.tensor([0.4],dtype=torch.float32)\n",
    "temp_tensor = torch.tensor([[[[1,2,3],[3,4,6],[2,6,9]]]],dtype=torch.float32,requires_grad=True)\n",
    "output = batchn(temp_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2752, 0.3168, 0.3584],\n",
      "          [0.3584, 0.4000, 0.4832],\n",
      "          [0.3168, 0.4832, 0.6080]]]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2753, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def compute_p1(a,index,eps):\n",
    "    a = a.flatten()\n",
    "    return (a[index]-torch.mean(a))/torch.sqrt(torch.var(input=a,unbiased=False)+eps) \n",
    "\n",
    "print(compute_p1(output,0,1e-5)*0.1+0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4000])\n",
      "tensor([1.5500])\n"
     ]
    }
   ],
   "source": [
    "print(batchn.running_mean)\n",
    "print(batchn.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = batchn(torch.tensor([[[[2,2,2],[2,2,2],[2,2,2]]]],dtype=torch.float32,requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5600])\n",
      "tensor([1.3950])\n"
     ]
    }
   ],
   "source": [
    "print(batchn.running_mean)\n",
    "print(batchn.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.56= 0.4*0.9 + 2*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后的Attention的梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个就寄了，softmax操作完全让大的数占据了所有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_input_tensor_k = torch.tensor([[[1,2,3],[4,5,6]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_input_tensor_q = torch.tensor([[[2,3,4],[5,6,7]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_input_tensor_v = torch.tensor([[[3,4,5],[6,7,8]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_matrix_before = torch.matmul(attention_input_tensor_q,attention_input_tensor_k.transpose(1,2))\n",
    "attention_matrix = torch.softmax((attention_matrix_before),dim=2)\n",
    "output = torch.matmul(attention_matrix,attention_input_tensor_v)\n",
    "target = torch.tensor([[[2.5,3.5,4.5],[5.5,6.5,7.5]]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[20., 47.],\n",
      "         [38., 92.]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[1.8795e-12, 1.0000e+00],\n",
      "         [3.5326e-24, 1.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[6., 7., 8.],\n",
      "         [6., 7., 8.]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attention_matrix_before)\n",
    "print(attention_matrix)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_input_tensor_k = torch.tensor([[[0.1,0.2,0.3],[0.4,0.5,0.6]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_input_tensor_q = torch.tensor([[[0.2,0.3,0.4],[0.5,0.6,0.7]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_input_tensor_v = torch.tensor([[[0.3,0.4,0.5],[0.6,0.7,0.8]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_matrix_before = torch.matmul(attention_input_tensor_q,attention_input_tensor_k.transpose(1,2))\n",
    "attention_matrix = torch.softmax((attention_matrix_before),dim=2)\n",
    "attention_matrix.retain_grad()\n",
    "output = torch.matmul(attention_matrix,attention_input_tensor_v)\n",
    "target = torch.tensor([[[0.25,0.35,0.45],[0.55,0.65,0.75]]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2000, 0.4700],\n",
      "         [0.3800, 0.9200]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[0.4329, 0.5671],\n",
      "         [0.3682, 0.6318]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.4701, 0.5701, 0.6701],\n",
      "         [0.4895, 0.5895, 0.6895]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attention_matrix_before)\n",
    "print(attention_matrix)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0881,  0.1541],\n",
      "         [-0.0242, -0.0423]]])\n"
     ]
    }
   ],
   "source": [
    "# print(attention_matrix.grad) 这个不会保留梯度\n",
    "print(attention_matrix.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0243, 0.0243, 0.0243],\n",
      "         [0.0289, 0.0289, 0.0289]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_input_tensor_v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "attention_input_tensor_k = torch.tensor([[[0.1,0.2,0.3],[0.4,0.5,0.6]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_input_tensor_q = torch.tensor([[[0.2,0.3,0.4],[0.5,0.6,0.7]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_input_tensor_v = torch.tensor([[[0.3,0.4,0.5],[0.6,0.7,0.8]]],dtype=torch.float32,requires_grad=True)\n",
    "attention_matrix_before = torch.matmul(attention_input_tensor_q,attention_input_tensor_k.transpose(1,2))\n",
    "attention_matrix_before.retain_grad()\n",
    "attention_matrix = attention_matrix_before * torch.tensor([[1,1],[1,0]]) # 每个attention矩阵至少保证有一个1\n",
    "attention_matrix = attention_matrix.masked_fill(attention_matrix==0,-1e9)\n",
    "attention_matrix.retain_grad()\n",
    "attention_matrix_sof = torch.softmax((attention_matrix),dim=2)\n",
    "attention_matrix_sof.retain_grad()\n",
    "output = torch.matmul(attention_matrix_sof,attention_input_tensor_v)\n",
    "output.retain_grad()\n",
    "target = torch.tensor([[[0.05,0.35,0.72],[0.1,0.65,0.85]]],dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_r = loss(output,target)\n",
    "loss_r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.4701, 0.5701, 0.6701],\n",
      "         [0.3000, 0.4000, 0.5000]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_r)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2000, 0.4700],\n",
      "         [0.3800, 0.9200]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[ 2.0000e-01,  4.7000e-01],\n",
      "         [ 3.8000e-01, -1.0000e+09]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.4329, 0.5671],\n",
      "         [1.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.4701, 0.5701, 0.6701],\n",
      "         [0.3000, 0.4000, 0.5000]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attention_matrix_before)\n",
    "print(attention_matrix)\n",
    "print(attention_matrix_sof)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0145,  0.0145],\n",
      "         [ 0.0000, -0.0000]]])\n",
      "-0.014484413984094495\n",
      "-0.014484357810000003\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp,sqrt,square\n",
    "print(attention_matrix.grad)\n",
    "print(0.0631*(exp(0.2)*exp(0.47)/(square(exp(0.2)+exp(0.47)) ))\n",
    "+0.1221*-1*(exp(0.2)*exp(0.47)/(square(exp(0.2)+exp(0.47)) )) ) # 这个值本来要算-0.0145的\n",
    "\n",
    "print(0.4329*(1-0.4329)*0.0631-0.1221*0.4329*0.5671)\n",
    "# 上面是原本的求导的计算，下面是用y的计算，可以发现简单许多，\n",
    "# 下面这个参数是反的是attention是2的时候碰巧的结果，很简单，一个大了就代表另外一个要小，\n",
    "# 或者说对于attention矩阵来说，显然是一个占比大就要代表另外一个占比小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0631,  0.1221],\n",
      "         [-0.0717, -0.1117]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_matrix_sof.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0145,  0.0145],\n",
      "         [ 0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_matrix_before.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1400,  0.0734, -0.0166],\n",
      "         [ 0.0667, -0.0833, -0.1167]]])\n"
     ]
    }
   ],
   "source": [
    "print(output.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1273, -0.0516, -0.1239],\n",
      "         [ 0.0794,  0.0416, -0.0094]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_input_tensor_v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.127306\n"
     ]
    }
   ],
   "source": [
    "# 计算v的左上角,v[0][0]的梯度包含2个成分，一个是attention_matrix_sof.grad[0][0]*v[0][0]+一个是attention_matrix_sof.grad[0][1]*v[1][0]的梯度\n",
    "print(0.14*0.4329+0.0667*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3502776/2938628293.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(temp_tensor,dtype=torch.float32,requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "temp_tensor = torch.tensor([[0.1,0.2,0.3],[0.4,0.5,0.6]],dtype=torch.float32,requires_grad=True)\n",
    "input = torch.tensor(temp_tensor,dtype=torch.float32,requires_grad=True)\n",
    "input.retain_grad()\n",
    "target = torch.tensor([1,0],dtype=torch.long)\n",
    "output = loss(input,target)\n",
    "\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1519, grad_fn=<NllLossBackward0>)\n",
      "1.151942848229244\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(output)\n",
    "print( (np.log(np.exp(0.2)/(np.exp(0.1)+np.exp(0.2)+np.exp(0.3)) ) +\n",
    "    np.log( np.exp(0.4)/(np.exp(0.4)+np.exp(0.5)+np.exp(0.6))))/2 * -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3502776/366074144.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(output.grad) # 这个grad应该是没意义的，\n"
     ]
    }
   ],
   "source": [
    "print(output.grad) # 这个grad应该是没意义的，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1503, -0.3339,  0.1836],\n",
      "        [-0.3497,  0.1661,  0.1836]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2538)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "temp_tensor = torch.tensor([ 5.52368e-02, -2.50854e-02,  5.28259e-02,  3.06549e-02, -4.99573e-02, -4.18701e-02, \n",
    "          6.87027e-03, -7.06787e-02, -6.45752e-02,  6.95801e-02, -3.74222e-03, -4.16946e-03,\n",
    "         -3.06244e-02, -1.17645e-02, -8.88062e-02, -4.68445e-02, -6.98242e-02,  1.39847e-02,\n",
    "          5.00870e-03,  8.09937e-02,  5.87769e-02,  8.59985e-02, -2.69928e-02, -7.68433e-02,\n",
    "          1.50833e-02,  1.95312e-02, -2.11334e-02, -1.12915e-01, -6.61621e-02,  2.77405e-02,\n",
    "         -5.17273e-02,  4.43115e-02, -1.12000e-01, -1.65558e-02, -1.28708e-02, -1.52664e-02,\n",
    "         -6.31104e-02,  1.99890e-02,  2.07825e-02, -1.57623e-02, -8.91113e-03, -4.70276e-02,\n",
    "         -4.63257e-02,  1.84937e-02, -1.77612e-02, -3.05023e-02,  3.26233e-02, -1.39923e-02,\n",
    "          6.85425e-02,  8.31909e-02, -3.85742e-02, -2.33154e-02,  4.29688e-02,  6.95801e-02,\n",
    "         -4.06799e-02,  9.59015e-03,  2.25830e-02, -1.33362e-02,  1.21002e-02,  3.44543e-02,\n",
    "          2.01263e-02, -4.10767e-02, -5.15137e-02,  1.06964e-02,  5.68008e-03,  4.59290e-02,\n",
    "         -1.23520e-02, -1.94397e-02, -5.14221e-02,  7.02667e-03, -2.49634e-02, -1.79749e-02,\n",
    "         -6.85883e-03, -1.07422e-01,  3.27148e-02, -3.48816e-02, -4.58374e-02,  3.47595e-02,\n",
    "         -4.47998e-02, -5.02014e-03, -3.49731e-02,  7.72858e-03,  1.11572e-01,  5.20630e-02,\n",
    "         -5.03235e-02,  3.71704e-02, -5.83191e-02,  3.20129e-02,  4.13513e-02,  1.00037e-01,\n",
    "         -3.31726e-02, -2.14081e-02,  3.90930e-02, -3.41187e-02, -5.11780e-02, -5.01709e-02,\n",
    "         -1.02478e-01,  7.74384e-03, -3.27110e-03, -5.39246e-02,  2.20947e-02, -5.85022e-02,\n",
    "          4.04053e-02,  7.19833e-03,  5.19714e-02, -6.25229e-03,  2.01111e-02,  1.97906e-02,\n",
    "         -3.72925e-02, -2.96631e-02,  6.02417e-02,  2.02484e-02, -6.28662e-02, -2.30713e-02,\n",
    "         -4.87137e-03,  3.42712e-02,  1.20300e-01,  7.21359e-03,  9.88770e-02, -9.86938e-02,\n",
    "          2.51465e-02,  2.43378e-02, -1.05133e-02,  3.02734e-02,  5.33142e-02,  2.97546e-02,\n",
    "         -5.49011e-02, -2.40021e-02,  6.90079e-03, -3.31421e-02, -6.93359e-02, -9.22241e-02,\n",
    "          1.73035e-02,  3.02582e-02, -2.77557e-02, -4.32129e-02, -2.70844e-02, -1.23291e-02,\n",
    "          6.59180e-02, -7.75814e-04,  1.47247e-02,  1.95770e-02, -2.70386e-02, -1.08109e-02,\n",
    "          3.12042e-03, -3.05939e-02,  1.72119e-02, -6.50024e-02, -7.36694e-02, -2.65503e-02,\n",
    "         -3.49731e-02,  2.12097e-02,  1.58024e-03, -3.22876e-02, -9.71069e-02, -1.50681e-02,\n",
    "         -5.07812e-02,  6.76270e-02,  6.61011e-02, -5.46455e-04, -6.47583e-02, -6.29883e-02,\n",
    "         -5.41687e-02,  5.63354e-02,  3.25317e-02, -2.65198e-02,  4.46167e-02,  1.78986e-02,\n",
    "         -5.47180e-02, -6.27441e-02,  5.29480e-02, -8.45337e-03, -3.25394e-03,  5.68848e-02,\n",
    "          8.28857e-02, -1.17371e-01, -3.75671e-02, -7.68280e-03, -1.62983e-03,  4.36707e-02,\n",
    "         -1.19781e-02,  8.75244e-02,  6.39648e-02, -6.74438e-02,  9.30786e-03,  3.13110e-02,\n",
    "          4.83398e-02, -6.61011e-02,  1.02692e-02, -3.97644e-02, -5.43213e-02, -3.95203e-02,\n",
    "          2.94495e-02, -1.75171e-02, -1.26648e-02, -3.20740e-02, -5.63354e-02, -7.43408e-02,\n",
    "          9.05762e-02,  6.45142e-02,  1.05362e-02,  4.33350e-02,  5.91736e-02, -5.79224e-02,\n",
    "         -3.01666e-02,  9.57489e-03, -3.85437e-02, -3.55835e-02,  7.91626e-02,  4.67529e-02,\n",
    "          1.09558e-02,  7.59125e-03,  8.66699e-02, -8.36792e-02,  5.86319e-03, -1.02520e-03,\n",
    "          2.79045e-03, -4.99573e-02, -3.33557e-02, -2.45667e-02,  2.89307e-02,  7.28226e-03,\n",
    "         -1.66779e-02,  3.90015e-02, -2.64282e-02, -3.36609e-02,  7.02515e-02,  5.65186e-02,\n",
    "          4.87061e-02, -4.70276e-02, -7.50122e-02, -3.09753e-02, -7.78198e-02,  7.48901e-02,\n",
    "          7.33032e-02,  4.56543e-02, -6.80542e-02, -5.38635e-03, -9.23157e-03, -1.58215e-03,\n",
    "         -8.38280e-04, -1.36948e-02, -1.11389e-01, -5.51758e-02, -1.17645e-02,  4.21753e-02,\n",
    "         -4.98047e-02, -2.92206e-02,  7.76367e-02,  2.48108e-02,  4.47083e-02,  5.96924e-02,\n",
    "          6.00281e-02,  7.60269e-03, -2.64740e-02,  7.45239e-02,  6.90918e-02, -9.14917e-02,\n",
    "         -3.82080e-02,  6.76727e-03, -4.24957e-03, -8.95996e-02,  1.88446e-02, -3.10059e-02,\n",
    "         -1.09329e-02,  4.12598e-02, -8.39996e-03, -1.79901e-02, -1.97906e-02, -2.75269e-02,\n",
    "         -4.48303e-02, -3.87878e-02, -9.39965e-05,  1.48468e-02,  7.73621e-03,  5.27344e-02,\n",
    "          4.87061e-02, -1.68324e-03, -4.93469e-02,  1.18637e-02,  6.78711e-02,  4.83093e-02,\n",
    "          1.06262e-01, -2.73438e-02,  8.83179e-02, -6.81152e-02,  8.99658e-02,  6.84204e-02,\n",
    "         -3.15857e-02, -4.80042e-02,  1.09009e-01,  8.88062e-02, -3.23486e-02,  2.93579e-02,\n",
    "          1.14632e-03,  3.13721e-02,  8.98743e-03, -3.39661e-02,  1.75018e-02, -2.54822e-02,\n",
    "         -2.18353e-02,  4.88281e-02, -9.44519e-03,  3.00751e-02, -5.59387e-02,  1.08185e-02,\n",
    "          5.20630e-02, -1.05476e-03,  2.16370e-02,  1.10779e-01,  1.03638e-01,  3.36609e-02,\n",
    "         -2.52228e-02,  1.06262e-01,  2.75612e-03,  8.75950e-04, -5.21240e-02,  1.02417e-01,\n",
    "         -1.98517e-02,  1.33133e-02,  5.69763e-02,  3.75977e-02,  2.47803e-02, -9.13239e-03,\n",
    "          9.50336e-04, -3.05481e-02,  2.83661e-02, -1.88904e-02, -7.23877e-02, -3.25012e-02,\n",
    "         -4.90570e-03, -3.18298e-02, -9.42993e-03,  6.92749e-02,  4.14124e-02, -4.92096e-03,\n",
    "         -1.17340e-02, -4.37012e-02, -9.52148e-03, -2.31476e-02,  3.05634e-02, -9.08203e-02,\n",
    "         -3.34930e-03, -3.71399e-02, -3.79028e-02,  4.61121e-02,  5.79071e-03, -4.86145e-02,\n",
    "          4.79736e-02, -9.68170e-03, -5.93567e-02, -3.50342e-02, -5.28870e-02, -1.47018e-02,\n",
    "         -1.76239e-02,  1.01242e-02, -1.43814e-02, -2.53296e-02,  3.50037e-02,  1.29013e-02,\n",
    "         -1.81122e-02, -4.58984e-02, -8.68530e-02, -2.20795e-02, -5.01404e-02,  2.17285e-02,\n",
    "          1.04370e-02,  4.89807e-03, -5.33447e-02,  3.37029e-03, -1.24054e-02,  2.16522e-02,\n",
    "         -6.07910e-02,  8.09479e-03,  1.08795e-02,  6.00281e-02, -1.83716e-02, -6.87256e-02,\n",
    "         -4.66614e-02, -1.35574e-02,  2.72827e-02, -1.79596e-02,  4.10156e-02,  3.90930e-02,\n",
    "          4.57458e-02, -5.37720e-02, -2.41394e-02,  1.32141e-02,  2.37274e-02,  7.43484e-03,\n",
    "         -1.35040e-03, -1.53122e-02, -1.15814e-02, -1.29013e-02, -6.54297e-02,  5.34973e-02,\n",
    "          6.50635e-02,  1.67084e-02,  8.39233e-03, -4.35791e-02, -2.01111e-02, -1.01013e-02,\n",
    "         -9.23462e-02,  1.93024e-02, -5.71594e-02,  6.78101e-02,  8.61816e-02,  3.44238e-02,\n",
    "         -5.07507e-02,  3.81470e-02, -4.11072e-02,  4.27856e-02,  4.69971e-02,  6.08444e-03,\n",
    "          3.23181e-02,  1.32828e-02, -3.93677e-02,  3.65601e-02,  2.58942e-02, -2.10266e-02,\n",
    "          1.65558e-02,  5.90210e-02, -2.21405e-02, -1.79291e-02,  1.43967e-02,  2.87933e-02,\n",
    "          5.01099e-02, -1.82953e-02, -4.83704e-03,  1.76544e-02,  3.35083e-02, -3.69873e-02,\n",
    "          2.19421e-02,  2.45972e-02,  3.44849e-02,  2.80914e-02, -2.58636e-02, -6.30493e-02,\n",
    "          2.13928e-02,  8.52051e-02, -8.13599e-02,  3.43933e-02, -3.05634e-02, -5.52368e-02,\n",
    "          5.83191e-02, -4.43726e-02,  1.67542e-02,  3.72620e-02, -9.33075e-03,  3.38440e-02,\n",
    "          1.34506e-02, -2.73438e-02, -5.63049e-02,  5.05676e-02, -2.05231e-02,  3.64685e-02,\n",
    "         -2.68250e-02,  2.32086e-02, -2.21100e-02, -1.94244e-02, -2.50397e-02, -1.12000e-02,\n",
    "          1.07346e-02,  6.36597e-02,  2.00348e-02, -4.50134e-02, -5.43518e-02,  6.31714e-03,\n",
    "          3.80554e-02, -4.49524e-02, -6.43311e-02,  3.49121e-02, -2.78015e-02,  4.44031e-02,\n",
    "          9.26971e-03, -1.40572e-03,  1.98364e-02, -7.37762e-03,  6.16760e-02,  1.12381e-02,\n",
    "         -1.24741e-02,  1.64795e-02, -1.65710e-02, -2.54669e-02, -5.05066e-02,  7.84874e-04,\n",
    "          1.73340e-02, -3.22571e-02,  4.48608e-02, -6.17065e-02, -1.03760e-02, -4.37317e-02,\n",
    "          2.97852e-02, -3.52783e-02, -3.10516e-02, -2.65503e-02, -5.57861e-02, -3.97644e-02,\n",
    "          9.50317e-02,  4.87671e-02,  8.34961e-02, -1.38626e-02,  8.22449e-03, -8.56781e-03,\n",
    "         -2.76184e-02,  4.27856e-02,  4.22287e-03, -8.17871e-02,  2.81525e-02, -3.29590e-02,\n",
    "         -1.16501e-02,  4.31519e-02,  6.73294e-03, -2.58179e-02,  5.02319e-02,  3.12500e-02,\n",
    "         -4.16870e-02,  5.12314e-03,  1.02463e-02,  2.59399e-02,  2.84882e-02, -3.54576e-03,\n",
    "          3.53394e-02, -1.21536e-02,  7.96509e-02, -1.57471e-02, -3.21350e-02,  6.77490e-02,\n",
    "         -2.22015e-02,  1.83563e-02, -3.03040e-02, -6.56738e-02,  6.31714e-02,  1.98483e-05,\n",
    "         -8.66699e-03,  5.18799e-02, -7.02381e-04,  4.79889e-03,  3.31726e-02, -8.79669e-03,\n",
    "         -9.81140e-03, -4.79736e-02, -3.99780e-02, -4.63867e-03,  1.14441e-02, -4.24194e-02,\n",
    "         -5.47485e-02, -2.96936e-02,  2.40936e-02,  1.82037e-02, -4.40979e-02,  2.31171e-02,\n",
    "         -4.38309e-03, -1.73035e-02, -5.41687e-02, -2.72522e-02,  2.55127e-02,  5.18799e-02,\n",
    "          2.65350e-02, -1.36032e-02,  8.99658e-02, -4.83093e-02,  9.22852e-02, -1.30692e-02,\n",
    "          5.24597e-02, -1.35269e-02,  1.04065e-02,  8.00171e-02, -5.22156e-02,  2.91748e-02,\n",
    "          9.12476e-02, -2.65198e-02,  5.83801e-02,  1.84174e-02,  3.20740e-02, -4.39453e-02,\n",
    "          6.59180e-02, -6.65283e-02, -7.61108e-02,  2.11945e-02,  1.33591e-02,  6.71387e-02,\n",
    "          6.42700e-02,  4.50134e-03, -6.71997e-02, -2.15759e-02,  1.78528e-02, -4.37317e-02,\n",
    "         -3.97949e-02,  2.70386e-02, -5.63354e-02,  5.91736e-02,  2.54822e-02, -1.44768e-03,\n",
    "          3.64075e-02,  2.35138e-02, -7.55615e-02,  6.29883e-02, -9.58862e-02, -1.19019e-02,\n",
    "         -6.92139e-02,  8.46558e-02, -2.65045e-02,  2.49939e-02,  8.42285e-02, -1.55258e-02,\n",
    "          6.65894e-02, -4.10461e-02,  3.14941e-02, -7.14111e-02,  6.62231e-02,  5.31311e-02,\n",
    "          8.01849e-03,  6.21338e-02,  1.69983e-02,  2.12250e-02,  4.64783e-02, -1.87378e-02,\n",
    "         -8.89893e-02,  5.00870e-03,  6.90918e-02,  1.21689e-02, -4.58679e-02, -2.77863e-02,\n",
    "         -8.34656e-03,  2.32086e-02,  2.72522e-02, -8.64868e-02,  7.65991e-02,  2.02942e-02,\n",
    "         -3.13721e-02, -1.70135e-02,  9.84192e-03,  1.49460e-02,  5.47180e-02, -4.22363e-02,\n",
    "          2.09503e-02,  3.82690e-02, -5.47409e-04,  6.78101e-02, -7.33032e-02, -4.37622e-02,\n",
    "         -2.40631e-02,  3.11737e-02,  2.63977e-02,  1.96228e-02, -7.91626e-02,  2.70996e-02,\n",
    "         -1.30463e-02,  1.33362e-02,  1.38321e-02,  2.13013e-02, -4.25720e-02, -3.56445e-02,\n",
    "         -7.39136e-02,  8.44574e-03, -4.88586e-02,  7.68433e-02,  1.24130e-02, -6.35376e-02,\n",
    "         -1.09100e-02,  8.18253e-04, -1.21613e-02, -1.15051e-01, -1.60217e-02,  4.51050e-02,\n",
    "         -2.60162e-02,  3.53394e-02, -7.78580e-03, -1.17950e-02,  1.84479e-02,  6.04553e-02,\n",
    "         -6.63452e-02,  3.23486e-02,  3.24707e-02, -1.69067e-02, -2.36511e-02,  9.51385e-03,\n",
    "          3.49731e-02,  1.73798e-02, -5.60303e-02, -3.26843e-02, -1.02768e-02,  4.38538e-02,\n",
    "         -2.02560e-03,  1.28662e-01, -1.87378e-02, -6.59943e-03, -3.55225e-02, -5.33447e-02,\n",
    "          3.56445e-02,  3.31497e-03,  4.05273e-02,  4.44031e-02,  1.07056e-01,  1.92871e-02,\n",
    "          1.35345e-02, -4.64172e-02,  5.79834e-02,  4.75769e-02, -1.12000e-01,  5.15137e-02,\n",
    "         -5.02930e-02,  4.33350e-02,  4.53186e-02,  7.86133e-02, -2.43988e-02,  1.11084e-01,\n",
    "          3.90320e-02, -1.17645e-02, -4.29993e-02, -2.92206e-03, -7.02286e-03, -6.06079e-02,\n",
    "         -3.98865e-02, -1.82190e-02,  6.70166e-02, -2.55890e-02,  7.94983e-03, -2.38800e-02,\n",
    "          2.77100e-02, -8.23975e-02,  4.40979e-03, -1.53427e-02,  6.99234e-03, -1.06689e-01,\n",
    "         -1.83411e-02, -5.11169e-02, -2.36969e-02,  7.56836e-03, -3.05023e-02, -6.08215e-02,\n",
    "         -4.26025e-02,  6.16760e-02, -7.75757e-02, -2.35443e-02,  9.43756e-03,  5.45044e-02,\n",
    "         -6.02722e-03, -9.07593e-02,  4.07715e-02,  6.39725e-03,  5.22995e-03,  5.26733e-02,\n",
    "          5.98145e-02,  2.94800e-02, -3.39661e-02,  3.18604e-02, -2.34680e-02,  5.89600e-02,\n",
    "          4.86145e-02,  2.43073e-02,  6.48117e-03, -3.49426e-02, -1.86157e-02, -2.78015e-02,\n",
    "         -9.52911e-03,  2.66571e-02,  4.06799e-02,  6.24084e-02,  2.65198e-02,  2.72560e-03,\n",
    "         -1.56212e-03, -3.39890e-03, -2.20795e-02,  7.77435e-03, -1.56708e-02, -6.53076e-02],dtype=torch.float32,)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
