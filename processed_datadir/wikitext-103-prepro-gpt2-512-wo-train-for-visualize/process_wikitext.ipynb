{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这个是处理GPT2-nano可视化注意力的数据集\n",
    "### 核心就是大于300的句子保留，其他截断，不合并短句子，只有验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load WikiText-103 dataset\n",
    "# dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "dataset = load_from_disk(\"../wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "Tokenizer max length:  1024\n",
      "Tokenizer max length after change:  512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../tokenizer_save/tokenizer-gpt2-512/tokenizer_config.json',\n",
       " '../tokenizer_save/tokenizer-gpt2-512/special_tokens_map.json',\n",
       " '../tokenizer_save/tokenizer-gpt2-512/vocab.json',\n",
       " '../tokenizer_save/tokenizer-gpt2-512/merges.txt',\n",
       " '../tokenizer_save/tokenizer-gpt2-512/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # change your own model\n",
    "print(tokenizer.vocab_size)\n",
    "print(\"Tokenizer max length: \", tokenizer.model_max_length)  # change your own model max input length\n",
    "tokenizer.model_max_length = 512\n",
    "print(\"Tokenizer max length after change: \", tokenizer.model_max_length) \n",
    "\n",
    "# Some tokenizer have a pad_token, which is used to pad a sequence up to max_length. But GPT2 tokenizer doesn't have it\n",
    "if tokenizer.pad_token is None:\n",
    "    # tokenizer.set_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# convenient function to load tokenizer next time\n",
    "tokenizer.save_pretrained(f\"../tokenizer_save/tokenizer-{tokenizer.name_or_path}-{tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edc6a24e678443888926a568d6d4ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make your own filter function\n",
    "def filter_function(example):  \n",
    "    return len(example['text'].split()) >= 300\n",
    "\n",
    "dataset_without_short = dataset.filter(filter_function,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19502\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_without_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22336f43e2964b03a3bcdd8c079a37aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/19502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define function to tokenize and encode text\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length)\n",
    "    \n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "preprocessed_dataset_without_short = dataset_without_short.map(preprocess_function, batched=True, num_proc=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f656b3fd02d64491a1797b58ec8a8931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "# Split preprocessed dataset into train, validation, and test sets\n",
    "# splits = preprocessed_dataset_without_short.train_test_split(test_size=0.1, shuffle=True)\n",
    "preprocessed_splits = DatasetDict({\n",
    "    \"validation\": preprocessed_dataset_without_short,\n",
    "    \"test\": load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc63ba3c9dd44aaa65f8775c9b58cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/19502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9a3043a9a24264917574681e0e9992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits.save_to_disk(\"../wikitext-103-prepro-gpt2-512-wo-train-for-visualize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
