{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load WikiText-103 dataset\n",
    "# dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "dataset = load_from_disk(\"../processed_datadir/wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801350\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' = Valkyria Chronicles III = \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel, Features\n",
    "story_data = {\n",
    "    \"text\":[]\n",
    "}\n",
    "index = 1\n",
    "while True:\n",
    "    if dataset[index][\"text\"].count(\"=\") == 2:\n",
    "        story_data[\"text\"].append(dataset[index][\"text\"])\n",
    "    else:\n",
    "        story_data[\"text\"][-1] = story_data[\"text\"][-1] + dataset[index][\"text\"]\n",
    "    index += 1\n",
    "    if index == len(dataset):\n",
    "        break\n",
    "    if index % 10000 == 1:\n",
    "        print(index)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(story_data[\"text\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29525\n"
     ]
    }
   ],
   "source": [
    "print(len(story_data[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e68c74abc54a05b0df48e8a45e9da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/29525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 创建数据集\n",
    "dataset = Dataset.from_dict(story_data)\n",
    "\n",
    "# 保存数据集\n",
    "dataset.save_to_disk(\"../processed_datadir/wikitext-103-story-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read next fast speed\n",
    "# dataset.save_to_disk(\"wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 1000):\n",
    "#     print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../tokenizer_save/tokenizer-gpt2-512/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404b5fca8a654ed79911fee075d811e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/29525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "# 害怕太多pad，导致模型只预测pad，所以只取最大长度的文本,后来一想这是不太可能的，attention会直接遮蔽，但是这样其实没有预测结束符\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True,   # 在这就不带文本了\n",
    "        max_length=tokenizer.model_max_length, return_overflowing_tokens=True,return_length=True)\n",
    "    input_ids_list = []\n",
    "    for length, input_ids in zip(tokenized_inputs[\"length\"], tokenized_inputs[\"input_ids\"]):\n",
    "        if length == tokenizer.model_max_length:\n",
    "            input_ids_list.append(input_ids)\n",
    "        \n",
    "    return {\"input_ids\": input_ids_list}\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = dataset.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split preprocessed dataset into train, validation, and test sets\n",
    "splits = tokenized_datasets.train_test_split(test_size=0.05)\n",
    "preprocessed_splits = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"validation\": splits[\"test\"],\n",
    "    \"test\": load_from_disk(\"../processed_datadir/wikitext-103-story-test/\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we can construct our own wikitext test data\n",
    "The biggest difference is that train and evaluate always stay with CrossEntry Loss, and won't care the real text effect, but text should have the overflowing tokens for GPT2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dbedb7fc7c458d8eddf7d960974707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    test_length = 512\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True,   # 在这就不带文本了\n",
    "        max_length=test_length, return_overflowing_tokens=True,return_length=True)\n",
    "    input_ids_list = []\n",
    "    prediction_labels = []\n",
    "    for length, input_ids in zip(tokenized_inputs[\"length\"], tokenized_inputs[\"input_ids\"]):\n",
    "        if length == test_length:\n",
    "            input_ids_list.append(input_ids[:test_length//2])\n",
    "            prediction_labels.append(input_ids[test_length//2:])\n",
    "        \n",
    "    return {\"input_ids\": input_ids_list,\"prediction_labels\": prediction_labels}\n",
    "\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(group_texts,batched=True, remove_columns=[\"text\"],num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(preprocessed_splits[\"test\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"overflowing_tokens_ids\"])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"num_truncated_tokens\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b520674e593b4eb18796bdbd741eccce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/204951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912424487f7144eaba5c1e2cc8aa4329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/204951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a3e84962744498a496cbcb5a61ec47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/10787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c9aae6681f4d42933d313cd5196080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ca9da9f59a4aa38f1c3b40d4a885a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits.save_to_disk(\"../processed_datadir/wikitext-103-gpt2-story-train512-test256\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
