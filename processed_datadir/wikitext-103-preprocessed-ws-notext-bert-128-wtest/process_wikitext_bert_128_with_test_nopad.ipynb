{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "dataset = load_from_disk('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load WikiText-103 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "dataset = load_from_disk(\"../processed_datadir/wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read next fast speed\n",
    "# dataset.save_to_disk(\"wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "Tokenizer max length:  512\n",
      "Tokenizer max length after change:  128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../tokenizer_save/tokenizer-bert-base-uncased-128/tokenizer_config.json',\n",
       " '../tokenizer_save/tokenizer-bert-base-uncased-128/special_tokens_map.json',\n",
       " '../tokenizer_save/tokenizer-bert-base-uncased-128/vocab.txt',\n",
       " '../tokenizer_save/tokenizer-bert-base-uncased-128/added_tokens.json',\n",
       " '../tokenizer_save/tokenizer-bert-base-uncased-128/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.vocab_size)\n",
    "print(\"Tokenizer max length: \", tokenizer.model_max_length)  # change your own model max input length\n",
    "tokenizer.model_max_length = 128\n",
    "print(\"Tokenizer max length after change: \", tokenizer.model_max_length) \n",
    "\n",
    "# Some tokenizer have a pad_token, which is used to pad a sequence up to max_length. But GPT2 tokenizer doesn't have it\n",
    "\n",
    "# convenient function to load tokenizer next time\n",
    "tokenizer.save_pretrained(f\"../tokenizer_save/tokenizer-{tokenizer.name_or_path}-{tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\"a \" * 10000,   # 果然给你拆分了，免得浪费文本\n",
    "        max_length=tokenizer.model_max_length, return_overflowing_tokens=True,return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d215d444a54623afad9bfe61f53789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "# 害怕太多pad，导致模型只预测pad，所以只取最大长度的文本\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True,   # 在这就不带文本了\n",
    "        max_length=tokenizer.model_max_length, return_overflowing_tokens=True,return_length=True)\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    \n",
    "    for length, input_ids in zip(tokenized_inputs[\"length\"], tokenized_inputs[\"input_ids\"]):\n",
    "        if length == tokenizer.model_max_length:\n",
    "            input_ids_list.append(input_ids)\n",
    "        # else:\n",
    "        #     input_ids_list.append(input_ids + [tokenizer.pad_token_id] * (tokenizer.model_max_length - length))\n",
    "    for length, token_type_ids in zip(tokenized_inputs[\"length\"], tokenized_inputs[\"token_type_ids\"]):\n",
    "        if length == tokenizer.model_max_length:\n",
    "            token_type_ids_list.append(token_type_ids)\n",
    "        # else:\n",
    "        #     token_type_ids_list.append(token_type_ids + [tokenizer.pad_token_type_id] * (tokenizer.model_max_length - length))\n",
    "    for length, attention_mask in zip(tokenized_inputs[\"length\"], tokenized_inputs[\"attention_mask\"]):\n",
    "        if length == tokenizer.model_max_length:\n",
    "            attention_mask_list.append(attention_mask)\n",
    "        # else:\n",
    "        #     attention_mask_list.append(attention_mask + [0] * (tokenizer.model_max_length - length))\n",
    "        \n",
    "    return {\"input_ids\": input_ids_list, \"token_type_ids\": token_type_ids_list, \"attention_mask\": attention_mask_list}\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = dataset.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets.features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501627\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801350\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(tokenized_datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # print(tokenizer.decode(tokenized_datasets[i][\"input_ids\"]))\n",
    "    print(tokenized_datasets[i])\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenized_datasets = load_from_disk(\"wikitext-103-raw-train-bert-half-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = tokenized_datasets.train_test_split(test_size=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "preprocessed_splits = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"validation\": splits[\"test\"],\n",
    "    \"test\": load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we can construct our own wikitext test data\n",
    "The biggest difference is that train and evaluate always stay with CrossEntry Loss, and won't care the real text effect, but text should have the overflowing tokens for GPT2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b95c3d04870417da9845c62fd1ca835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(group_texts,batched=True, remove_columns=[\"text\"],num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'overflow_to_sample_mapping': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_splits[\"test\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2728, 8945, 11314, 2121, 2003, 2019, 2394, 2143, 1010, 2547, 1998, 3004, 3364, 1012, 2002, 2018, 1037, 4113, 1030, 1011, 1030, 4626, 2535, 2006, 1996, 2547, 2186, 1996, 3021, 1999, 2456, 1012, 2023, 2001, 2628, 2011, 1037, 4626, 2535, 1999, 1996, 2377, 22914, 2015, 2517, 2011, 4079, 15037, 1010, 2029, 2001, 2864, 1999, 2541, 2012, 1996, 2548, 2457, 3004, 1012, 2002, 2018, 1037, 4113, 2535, 1999, 1996, 2547, 2186, 3648, 2198, 15046, 1999, 2526, 1012, 1999, 2432, 8945, 11314, 2121, 5565, 1037, 2535, 2004, 1000, 7010, 1000, 1999, 1996, 2792, 1000, 11389, 1005, 1055, 2466, 1000, 1997, 1996, 2547, 2186, 1996, 2146, 3813, 1025, 2002, 5652, 4077, 5889, 2928, 2844, 1998, 7256, 6213, 2072, 1012, 2002, 2001, 3459, 1999, 1996, 2384, 3004, 5453, 1997, 1996, 5170, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 2294, 1010, 8945, 11314, 2121, 5652, 4077, 1059, 24158, 14238, 1999, 1996, 2377, 9068, 2517, 2011, 2928, 10000, 7100, 1012, 2002, 2596, 2006, 1037, 2294, 2792, 1997, 1996, 2547, 2186, 1010, 7435, 1010, 2628, 2011, 1037, 2535, 1999, 1996, 2289, 3004, 2537, 1997, 2129, 2000, 8364, 2856, 2011, 15293, 24400, 1012, 2129, 2000, 8364, 2001, 2864, 2012, 5747, 3004, 1999, 1996, 2414, 5538, 1997, 28420, 1998, 21703, 1012, 8945, 11314, 2121, 5652, 1999, 2048, 3152, 1999, 2263, 1010, 11695, 13742, 2011, 12127, 3000, 6506, 3775, 1010, 1998, 20325, 8595, 2856, 2011, 19330, 2135, 13934, 1012, 1999, 2089, 2263, 1010, 8945, 11314, 2121, 2081, 1037, 4113, 3311, 2006, 1037, 2048, 1030, 1011, 1030, 2112, 2792, 8115, 1997, 1996, 2547, 2186, 12447, 1996, 2757, 1010, 2628, 2011, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 2456, 8945, 11314, 2121, 2018, 1037, 4113, 1030, 1011, 1030, 4626, 2535, 2006, 1996, 2547, 2186, 1996, 3021, 1025, 2002, 6791, 1000, 3660, 20803, 1000, 1999, 1996, 2792, 1010, 1000, 1999, 3647, 2398, 1000, 1012, 8945, 11314, 2121, 5652, 2004, 1000, 3660, 1000, 1999, 1996, 2377, 22914, 2015, 2517, 2011, 4079, 15037, 1010, 2029, 2001, 2864, 1999, 2541, 2012, 1996, 2548, 2457, 3004, 1012, 1037, 3319, 1997, 8945, 11314, 2121, 1005, 1055, 2836, 1999, 1996, 2981, 2006, 4465, 2649, 2032, 2004, 1000, 27762, 24060, 1000, 1999, 1996, 2535, 1010, 1998, 2002, 2363, 4187, 4391, 1999, 1996, 9536, 1010, 1998, 3944, 3115, 1012, 2002, 2596, 1999, 1996, 2547, 2186, 3648, 2198, 15046, 1999, 2526, 2004, 1000, 5587, 6633, 2849, 6590, 3351, 1000, 1999, 1996, 2792, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2002, 2018, 1037, 10694, 2535, 1999, 2494, 2006, 2048, 4178, 1997, 1996, 3021, 1010, 2004, 2839, 1000, 6720, 3976, 1000, 1012, 1999, 2432, 8945, 11314, 2121, 5565, 1037, 2535, 2004, 1000, 7010, 1000, 1999, 1996, 2792, 1000, 11389, 1005, 1055, 2466, 1000, 1997, 1996, 2547, 2186, 1996, 2146, 3813, 1025, 2002, 5652, 4077, 5889, 2928, 2844, 1998, 7256, 6213, 2072, 1012, 8945, 11314, 2121, 5652, 2004, 1000, 12270, 1000, 1010, 1999, 1996, 2384, 3004, 5453, 1997, 1996, 5170, 20608, 2377, 8714, 6519, 1012, 2009, 2001, 2864, 2012, 1996, 6943, 3004, 1999, 10221, 1010, 1998, 1996, 2273, 3771, 7967, 4713, 1999, 2414, 1012, 2002, 2001, 2856, 2011, 2198, 14381, 1998, 5652, 4077, 3841, 1059, 24158, 14238, 1010, 8683, 23564, 4143, 1010, 4302, 5982, 1010, 9443, 1037, 16363, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 2294, 8945, 11314, 2121, 5652, 1999, 1996, 2377, 9068, 2517, 2011, 2928, 10000, 7100, 1012, 1996, 2377, 2001, 2112, 1997, 1037, 2186, 2029, 2956, 2367, 11170, 2015, 1010, 4159, 6402, 1013, 11834, 9954, 1013, 9068, 1012, 1999, 1037, 2294, 4357, 1010, 3507, 3364, 3841, 1059, 24158, 14238, 4453, 8945, 11314, 2121, 2004, 2028, 1997, 2010, 5440, 2522, 1030, 1011, 1030, 3340, 1024, 1000, 1045, 3866, 2551, 2007, 1037, 3124, 2170, 2728, 8945, 11314, 2121, 1010, 2040, 2001, 1999, 1996, 6420, 3021, 1997, 6402, 1010, 11834, 9954, 1998, 9068, 2012, 1996, 2120, 1012, 2002, 2209, 2026, 2567, 1999, 8714, 6519, 1012, 1000, 2002, 6791, 1000, 4463, 7482, 1000, 2006, 1996, 2294, 2792, 1997, 1996, 2547, 2186, 1010, 7435, 1010, 4159, 1000, 2242, 1045, 8823, 1000, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 8945, 11314, 2121, 5652, 1999, 2048, 3152, 1999, 2263, 1010, 11695, 13742, 2011, 12127, 3000, 6506, 3775, 1010, 1998, 20325, 8595, 2856, 2011, 19330, 2135, 13934, 1012, 8945, 11314, 2121, 6791, 1037, 2839, 2315, 1000, 5977, 1000, 1999, 20325, 8595, 1010, 2040, 22073, 2247, 2007, 2839, 1000, 6498, 1000, 2004, 1996, 1000, 4251, 2567, 1012, 1012, 1012, 2040, 4978, 2009, 2125, 2007, 17214, 4328, 1000, 1012, 8945, 11314, 2121, 4113, 5652, 2006, 1037, 2048, 1030, 1011, 1030, 2112, 2792, 8115, 1000, 8710, 1000, 1999, 2089, 2263, 1997, 1996, 2547, 2186, 12447, 1996, 2757, 2004, 2839, 1000, 5261, 6203, 4181, 1000, 1012, 2002, 2596, 2006, 1996, 2547, 2186, 8643, 2004, 1000, 6606, 1000, 1999, 2281, 2263, 1012, 2002, 2018, 1037, 10694, 2535, 1999, 2702, 4178, 1997, 1996, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3151, 2822, 4706, 6256, 13155, 1996, 2166, 1997, 1996, 3166, 2043, 25455, 1037, 2147, 1010, 1037, 3218, 2029, 9658, 7908, 12332, 2000, 1000, 1996, 2485, 6971, 2008, 3151, 2822, 2245, 13433, 28032, 2015, 2090, 2396, 1998, 16561, 1000, 1012, 2144, 2116, 1997, 4241, 11865, 1005, 1055, 5878, 3444, 16561, 1998, 2381, 1010, 2023, 3218, 2003, 3391, 2590, 1012, 2178, 3114, 1010, 4453, 2011, 1996, 2822, 5272, 2520, 5112, 1010, 2003, 2008, 2822, 5878, 2024, 4050, 9530, 18380, 1010, 18168, 12474, 2075, 6123, 2008, 2453, 2022, 7882, 1010, 2021, 2029, 2019, 6727, 3824, 2071, 2022, 5071, 2000, 2113, 1012, 2005, 2715, 2530, 8141, 1010, 1000, 1996, 2625, 14125, 2057, 2113, 1996, 2051, 1010, 1996, 2173, 1998, 1996, 6214, 1999, 1996, 4281, 1010, 1996, 2062, 20090, 2057, 2024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 1996, 2220, 28004, 2015, 1010, 2002, 7837, 1999, 1996, 28091, 1013, 26805, 2181, 1025, 2010, 5700, 6405, 5961, 1010, 7851, 1037, 4623, 5049, 1010, 2003, 2245, 2000, 3058, 2013, 1996, 2203, 1997, 2023, 2558, 1010, 2105, 6421, 2629, 1012, 1999, 2008, 2095, 1010, 2002, 2165, 1996, 2942, 2326, 11360, 1010, 3497, 1999, 11132, 1005, 2019, 1012, 2002, 3478, 1010, 2000, 2010, 4474, 1998, 2008, 1997, 4693, 1997, 2101, 4401, 1012, 5112, 14730, 2008, 2002, 2763, 3478, 2138, 2010, 12388, 2806, 2012, 1996, 2051, 2001, 2205, 9742, 1998, 14485, 1010, 2096, 16480, 2226, 6083, 2010, 4945, 2000, 8754, 21466, 7264, 1999, 1996, 3007, 2089, 2031, 2042, 2000, 7499, 1012, 2044, 2023, 4945, 1010, 2002, 2253, 2067, 2000, 7118, 1010, 2023, 2051, 2105, 25768, 1998, 2002, 19205, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 1996, 7114, 1997, 6356, 2549, 1010, 2002, 2777, 5622, 21790, 1006, 5622, 13433, 1007, 2005, 1996, 2034, 2051, 1010, 1998, 1996, 2048, 9736, 2719, 1037, 6860, 1012, 2585, 2402, 5577, 2023, 2004, 1000, 1996, 2087, 3278, 4289, 3512, 5783, 1999, 4241, 11865, 1005, 1055, 6018, 2458, 1000, 2138, 2009, 2435, 2032, 1037, 2542, 2742, 1997, 1996, 28667, 7393, 3512, 4802, 1030, 1011, 1030, 6288, 2166, 2000, 2029, 2002, 2001, 6296, 2044, 2010, 4945, 1999, 1996, 2942, 2326, 11360, 1012, 1996, 3276, 2001, 5399, 2028, 1030, 1011, 1030, 11536, 1010, 2174, 1012, 4241, 11865, 2001, 2011, 2070, 2086, 1996, 3920, 1010, 2096, 5622, 21790, 2001, 2525, 1037, 13805, 2732, 1012, 2057, 2031, 4376, 5878, 2000, 2030, 2055, 5622, 21790, 2013, 1996, 3920, 4802, 1010, 2021, 2069, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 6356, 2575, 1010, 2002, 2333, 2000, 1996, 3007, 1999, 2019, 3535, 2000, 24501, 3126, 2890, 6593, 2010, 2880, 2476, 1012, 2002, 2165, 1996, 2942, 2326, 11360, 1037, 2117, 2051, 2076, 1996, 2206, 2095, 1010, 2021, 2035, 1996, 5347, 2020, 3478, 2011, 1996, 3539, 2704, 1006, 4593, 1999, 2344, 2000, 4652, 1996, 14053, 1997, 2825, 9169, 1007, 1012, 2002, 2196, 2153, 4692, 1996, 14912, 1010, 2612, 9964, 2075, 1996, 3750, 3495, 1999, 4293, 2487, 1010, 4293, 2549, 1998, 2763, 2153, 1999, 4293, 2629, 1012, 2002, 2496, 2105, 4293, 2475, 1010, 1998, 2011, 4293, 2581, 1996, 3232, 2018, 2018, 2274, 2336, 1517, 2093, 4124, 1998, 2048, 5727, 1517, 2021, 2028, 1997, 1996, 4124, 2351, 1999, 22813, 1999, 4293, 2629, 1012, 2013, 4293, 2549, 2002, 2211, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(preprocessed_splits[\"test\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"overflowing_tokens_ids\"])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"num_truncated_tokens\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bbe9c70dd94f75b8e7b7323f58880d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/476545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239690fca9084c7188a9e27289bfefd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/476545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab13f45ab2644c19443bdf2221bb4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/25082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34eeb438d4f463abeb2a42f14d08334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e42b5e4a4e14c86ad7c64c6188f9594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits.save_to_disk(\"../processed_datadir/wikitext-103-preprocessed-ws-notext-bert-128-wtest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
