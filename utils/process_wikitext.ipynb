{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load WikiText-103 dataset\n",
    "# dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "dataset = load_from_disk(\"../wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read next fast speed\n",
    "# dataset.save_to_disk(\"wikitext-103-raw-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "Tokenizer max length:  1024\n",
      "Tokenizer max length after change:  256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer_save/tokenizer-gpt2-256/tokenizer_config.json',\n",
       " './tokenizer_save/tokenizer-gpt2-256/special_tokens_map.json',\n",
       " './tokenizer_save/tokenizer-gpt2-256/vocab.json',\n",
       " './tokenizer_save/tokenizer-gpt2-256/merges.txt',\n",
       " './tokenizer_save/tokenizer-gpt2-256/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # change your own model\n",
    "print(tokenizer.vocab_size)\n",
    "print(\"Tokenizer max length: \", tokenizer.model_max_length)  # change your own model max input length\n",
    "tokenizer.model_max_length = 256\n",
    "print(\"Tokenizer max length after change: \", tokenizer.model_max_length) \n",
    "\n",
    "# Some tokenizer have a pad_token, which is used to pad a sequence up to max_length. But GPT2 tokenizer doesn't have it\n",
    "if tokenizer.pad_token is None:\n",
    "    # tokenizer.set_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# convenient function to load tokenizer next time\n",
    "tokenizer.save_pretrained(f\"./tokenizer_save/tokenizer-{tokenizer.name_or_path}-{tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d503a6c6ba8f4aa9be4713cbd05bcb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make your own filter function\n",
    "def filter_function(example):  \n",
    "    return len(example['text'].split()) >= 100\n",
    "\n",
    "dataset_without_short = dataset.filter(filter_function,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3ac265f8954c7f9b9066ff01bbd867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/469993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/songx_lab/cse12012530/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "# Define function to tokenize and encode text\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length)\n",
    "    \n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "preprocessed_dataset_without_short = dataset_without_short.map(preprocess_function, batched=True, num_proc=4)\n",
    "\n",
    "# Split preprocessed dataset into train, validation, and test sets\n",
    "splits = preprocessed_dataset_without_short.train_test_split(test_size=0.1, shuffle=True)\n",
    "preprocessed_splits = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"validation\": splits[\"test\"],\n",
    "    \"test\": load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we can construct our own wikitext test data\n",
    "The biggest difference is that train and evaluate always stay with CrossEntry Loss, and won't care the real text effect, but text should have the overflowing tokens for GPT2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7a963c18a247adaac2dfa1532049b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(example):\n",
    "    return_dic = tokenizer.__call__(example[\"text\"], padding=\"max_length\", truncation=True,   # 在这就不带文本了\n",
    "        max_length=tokenizer.model_max_length, return_overflowing_tokens=True) \n",
    "\n",
    "    # if len(return_dic[\"overflowing_tokens\"]) > 0:\n",
    "    #     return_dic[\"overflow_text\"] = tokenizer.decode(return_dic[\"overflowing_tokens\"])\n",
    "    # else:\n",
    "    #     return_dic[\"overflow_text\"] = \"\"\n",
    "    # return_dic[\"prompt\"] = tokenizer.decode(return_dic[\"input_ids\"])\n",
    "    return return_dic\n",
    "#Apply preprocessing to dataset\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(preprocess_function,batched=True, remove_columns=[\"text\"],num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f94a8f17ea34099bd01a9c154490e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def filter_function(example):  \n",
    "    return len(example['overflowing_tokens']) > 20 and len(example['overflowing_tokens']) < 300 \n",
    "\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].filter(filter_function,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251c583ad1e644dfa94b201ce1421316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function_truncate(example, max_over_length=128):\n",
    "    # if isinstance(example[\"overflowing_tokens\"], list):\n",
    "    #     example[\"overflowing_tokens\"] = example[\"overflowing_tokens\"].truncate(max_over_length)\n",
    "    # else:\n",
    "    # print(\"type: \",type(example[\"overflowing_tokens\"]))\n",
    "    def complete_list(lst, length, value=tokenizer.eos_token_id):\n",
    "        l = len(lst)\n",
    "        return lst + [value] * (length - l)\n",
    "    if len(example[\"overflowing_tokens\"]) > max_over_length:\n",
    "        example[\"overflowing_tokens\"] = example[\"overflowing_tokens\"][:max_over_length]\n",
    "    else:\n",
    "        example[\"overflowing_tokens\"] = complete_list(example[\"overflowing_tokens\"], max_over_length)\n",
    "    return example\n",
    "\n",
    "preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].map(preprocess_function_truncate, batched=False, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"overflowing_tokens_ids\"])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    preprocessed_splits[\"test\"] = preprocessed_splits[\"test\"].remove_columns([\"num_truncated_tokens\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e62672609b4c368ef269de96eb1dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/422993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa4745e4af042b387e4e71101ac720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/422993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c30dd5b84eb4778ad141dc6d5568c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/47000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762c41b110304da896068e525b5ad6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/47000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3b20f4fe9c479c8cc677b75c294b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits.save_to_disk(\"../wikitext-103-preprocessed-ws-notext-gpt2-256-wtest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
