{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read next fast speed\n",
    "dataset = load_from_disk(\"../processed_datadir/wikitext-103-story-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None)}\n",
      "29525\n"
     ]
    }
   ],
   "source": [
    "print(dataset.features)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     print(len(dataset[i]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# import numpy as np\n",
    "\n",
    "# # 初始化tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4468 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 2\n",
      "attention_mask 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "temp = tokenizer.batch_encode_plus(dataset[0:2]['text'],truncation=False)\n",
    "# print(temp)                        # 这样搞直接把20000多的文本截取到只剩下2048\n",
    "for k in temp.keys():\n",
    "        if isinstance(temp[k],list):\n",
    "                print(k, len(temp[k]))\n",
    "        else:\n",
    "                print(k, temp[k])\n",
    "# print(temp['input_ids'])\n",
    "# print(temp['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "The max length for the tokenizer is: 2048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae260eec8f6453cb5d4990ec4138817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/29525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "successsuccess\n",
      "\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_proc = multiprocessing.cpu_count() - 4\n",
    "# num_proc = 1\n",
    "print(num_proc)\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    # print(len(examples[\"text\"]))\n",
    "    # temp_string_list = []\n",
    "    # for j in range(len(examples[\"text\"])):\n",
    "    #     text = examples[\"text\"][j]\n",
    "    #     temp_string_list.extend([text[i:i+tokenizer.model_max_length] for i in range(0, len(text), tokenizer.model_max_length)])\n",
    "    # print(\"len of examples['text']:\", len(examples[\"text\"]))\n",
    "    # print(\"len of examples['text'][0]:\", len(examples[\"text\"][0]))\n",
    "    tokenized_inputs = tokenizer.batch_encode_plus(examples[\"text\"], truncation = False)\n",
    "    input_ids_list = []\n",
    "    # token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    # print(\"len of tokenized_inputs['input_ids']:\", len(tokenized_inputs[\"input_ids\"]))\n",
    "    # print(\"len of tokenized_inputs['input_ids'][0]:\", len(tokenized_inputs[\"input_ids\"][0]))\n",
    "    # print(\"len of tokenized_inputs['attention_mask']:\", len(tokenized_inputs[\"attention_mask\"]))\n",
    "    # print(\"len of tokenized_inputs['attention_mask'][0]:\", len(tokenized_inputs[\"attention_mask\"][0]))\n",
    "\n",
    "\n",
    "    \n",
    "    for input_ids, attention_mask in zip(tokenized_inputs[\"input_ids\"], tokenized_inputs[\"attention_mask\"]):\n",
    "        # print(input_ids)\n",
    "        # print(attention_mask)\n",
    "        # 按照tokenizer.model_max_length的长度进行切断\n",
    "        chunks_input_ids = [input_ids[i:i + tokenizer.model_max_length] for i in range(0, len(input_ids), tokenizer.model_max_length)]\n",
    "        chunks_attention_mask = [attention_mask[i:i + tokenizer.model_max_length] for i in range(0, len(attention_mask), tokenizer.model_max_length)]\n",
    "\n",
    "        # 对于不足tokenizer.model_max_length的部分进行补全\n",
    "        chunks_input_ids = [chunk + [tokenizer.pad_token_id] * (tokenizer.model_max_length - len(chunk)) for chunk in chunks_input_ids]\n",
    "        chunks_attention_mask = [chunk + [0] * (tokenizer.model_max_length - len(chunk)) for chunk in chunks_attention_mask]\n",
    "\n",
    "        input_ids_list.extend(chunks_input_ids)\n",
    "        attention_mask_list.extend(chunks_attention_mask)\n",
    "    print(\"success\")\n",
    "    # return {\"input_ids\": input_ids_list, \"token_type_ids\": token_type_ids_list, \"attention_mask\": attention_mask_list}\n",
    "    return {\"input_ids\": input_ids_list, \"attention_mask\": attention_mask_list}\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = dataset.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72224\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets)) # 这个理论上wikitext103数据集要在27万左右，如果是处理GPT只要7万"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(tokenized_datasets)):\n",
    "    # print(tokenized_datasets[i])\n",
    "    for k in tokenized_datasets[i]:\n",
    "        # print(k)\n",
    "        # print(len(tokenized_datasets[i][k]))\n",
    "        if len(tokenized_datasets[i][k]) != tokenizer.model_max_length:\n",
    "            print(\"error\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split preprocessed dataset into train, validation, and test sets\n",
    "splits = tokenized_datasets.train_test_split(test_size=0.03)\n",
    "preprocessed_splits = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"validation\": splits[\"test\"],\n",
    "    \"test\": load_from_disk(\"../processed_datadir/wikitext-103-story-test/\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af16fc061164ae791fd6c629052356d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/70057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4d482ea7d14ec09f69078e7a2d3ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/70057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0babfddaa744408e9957e24068d124d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fc9de360a34eeb85db3a1333e8ee89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8636028e7c5d4396a352b97951dca3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_splits.save_to_disk(\"../processed_datadir2/wikitext-103-story-gpt2-2048/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70057\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_splits[\"train\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e895937462a0f081aaf35a1b00d743630ae75cf7f3d3dbe937ee3f340de9cfa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
